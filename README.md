# AXIOM OS â€” PoC v0.4: CUDA â†’ AXIR â†’ HIP Translator (Universal Pivot)

**Vision**  
Free AI from the CUDA lock-in.  
AXIOM OS introduces **AXIR**, a universal intermediate representation that lets the same AI code run across NVIDIA, AMD, Intel, and more.  

CUDA is just the first step: Axiomos is building the **universal AI operating system**, breaking hardware silos and enabling true portability.

 Official website: [axiomos.ai](https://axiomos.ai)

---

## About this Repository

This repository contains the **Proof of Concept v0.4** of AXIOM OS.

- `axiomify.py` â†’ Python toolchain with two stages:
  - **Front-end**: CUDA â†’ AXIR (JSON intermediate representation)
  - **Back-end**: AXIR â†’ HIP (AMD ROCm), or other targets in the future
  - Shortcut: `--hip-direct` for CUDA â†’ HIP directly

- `hip2cuda.py` â†’ Reverse translator (HIP â†’ CUDA) to run HIP-generated code on NVIDIA GPUs (Colab, local).

- `demo.sh` â†’ End-to-end demo (CUDA â†’ AXIR â†’ HIP â†’ CUDA â†’ Run).

- `examples/` â†’ Sample CUDA kernels (vector addition, async/streams).

- `docs/` â†’ Roadmap PDF and visual timeline.

---

##  Quick Usage

### 1. CUDA â†’ AXIR
```bash
python3 axiomify.py examples/mini_vector_add.cu --emit-axir -o mini_vector_add.axir.json

2. AXIR â†’ HIP
python3 axiomify.py mini_vector_add.axir.json --from-axir -o mini_vector_add_glue.hip.cpp

3. CUDA â†’ HIP (direct shortcut)
python3 axiomify.py examples/mini_vector_add.cu --hip-direct -o mini_vector_add_hip.cpp

 Showtime Demo (end-to-end)

For the full pipeline (CUDA â†’ AXIR â†’ HIP â†’ back to CUDA â†’ run):

chmod +x demo.sh
./demo.sh


This will display:

The original CUDA code

The AXIR pivot JSON

The HIP (glue and full)

The regenerated CUDA

The final execution result:

RÃ©sultat : 11 22 33 44 55

 Repository Structure
axiomos-poc/
 â”œâ”€ axiomify.py          # CUDA <-> AXIR <-> HIP
 â”œâ”€ hip2cuda.py          # HIP -> CUDA reconversion
 â”œâ”€ demo.sh              # End-to-end demo
 â”œâ”€ examples/
 â”‚   â”œâ”€ mini_vector_add.cu
 â”‚   â”œâ”€ mini_async_copy.cu
 â”‚   â””â”€ README.md
 â”œâ”€ docs/
 â”‚   â”œâ”€ axiom_os_roadmap_visual.pdf
 â”‚   â””â”€ axiom_os_timeline.png
 â”œâ”€ LICENSE
 â””â”€ .gitignore

ğŸ—º Roadmap (excerpt)

Phase 0: CUDA â†’ HIP PoC (regex-based) âœ…

Phase 1: Broader API coverage (streams/events) âœ…

Phase 2 (v0.4): AXIR intermediate representation introduced âœ…

Phase 3: Intel backend (SYCL/oneAPI)

Phase 4: Framework bridges (PyTorch ops)

Phase 5: Performance & auto-tuning

Phase 6: Pro Edition (Analyzer, Portability Score)

See docs/axiom_os_roadmap_visual.pdf for details.

âš ï¸ Known Limitations (v0.4)

AXIR is still minimal (covers malloc, memcpy, kernel launch, sync, free).

Only basic CUDA runtime APIs are supported.

Libraries (cuBLAS/cuDNN) not yet handled.

Performance optimization is not the goal of this PoC.

 License

MIT â€” free to use, modify, and redistribute.

With this PoC, AXIOM OS takes its first concrete step toward becoming the universal AI operating system â€” the Windows of AI middleware.
